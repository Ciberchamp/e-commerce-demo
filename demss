11111111111111111111111111111111111111111111111111111
import mysql.connector
import csv

# MySQL connection config
config = {
    'user': 'root',
    'password': 'Usha.0812',
    'host': 'localhost',
    'database': 'dat'
}

# Table and CSV
table_name = 'dm'
csv_file = 'data.csv'

# Step 1: Extract - Read CSV file
def extract_csv(file):
    with open(file, mode='r') as f:
        return list(csv.reader(f))[1:]  # skip header

# Step 2: Transform - Remove duplicate rows
def transform(data):
    return list({tuple(row) for row in data})  # convert to set of tuples to remove duplicates

# Step 3: Load - Insert data into MySQL
def load_to_db(data):
    conn = mysql.connector.connect(**config)
    cursor = conn.cursor()
    cursor.execute(f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            id INT PRIMARY KEY,
            name VARCHAR(255),
            age INT
        )
    """)
    cursor.execute(f"DELETE FROM {table_name}")  # Clear existing data for simplicity
    cursor.executemany(f"INSERT INTO {table_name} (id, name, age) VALUES (%s, %s, %s)", data)
    conn.commit()
    print("ETL completed: Data loaded to DB.")
    cursor.close()
    conn.close()

# Main ETL flow
data = extract_csv(csv_file)
clean_data = transform(data)
load_to_db(clean_data)




22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
import pandas as pd

# Sample dataset 1 (with missing values)
df1 = pd.DataFrame({
    'ID': [1, 2, 3, 4],
    'Name': ['Alice', 'Bob', None, 'David'],
    'Age': [25, None, 30, 22]
})

# Sample dataset 2
df2 = pd.DataFrame({
    'ID': [3, 4, 5, 6],
    'Salary': [50000, 60000, 55000, 70000]
})

# **1. Data Cleaning** - Fill missing values
df1.fillna({'Name': 'Unknown', 'Age': df1['Age'].mean()}, inplace=True)

# **2. Data Integration & Transformation** - Merge datasets on 'ID'
df_merged = pd.merge(df1, df2, on='ID', how='outer')

# **3. Data Reduction** - Drop 'Age' column to reduce dimensions
df_reduced = df_merged.drop(columns=['Age'])

# Display final processed data
print(df_reduced)




33333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333
import pandas as pd

# Sample Data
df = pd.DataFrame({
    'Age': [15, 22, 35, 42, 51, 63, 75]
    })

# -------------------------------
# Data Discretization: Bin 'Age' into categories
# -------------------------------
df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 18, 40, 60, 100], labels=['Teen', 'Young Adult', 'Middle-Aged', 'Senior'])

# -------------------------------
# Concept Hierarchy Generation
# -------------------------------
hierarchy = {
    'Teen': 'Young',
    'Young Adult': 'Young',
    'Middle-Aged': 'Adult',
    'Senior': 'Elder'
}
df['Hierarchy'] = df['AgeGroup'].map(hierarchy)

print(df)


4v4444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444

from sklearn.datasets import make_classification
from imblearn.over_sampling import SMOTE
import numpy as np

# Generate an imbalanced dataset
X, y = make_classification(n_classes=2, weights=[0.9, 0.1], n_samples=1000, random_state=42)
print("Class distribution before balancing:", np.bincount(y))

# Apply SMOTE for balancing
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

print("Class distribution after balancing:", np.bincount(y_resampled))



555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555

import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# Sample dataset
dataset = [
    ['milk', 'bread', 'nuts', 'apple'],
    ['milk', 'bread', 'nuts'],
    ['milk', 'bread'],
    ['milk', 'apple'],
    ['bread', 'nuts', 'apple'],
    ['milk', 'bread', 'apple'],
    ['bread', 'nuts'],
]

# Convert transactions to DataFrame
te = TransactionEncoder()
df = pd.DataFrame(te.fit_transform(dataset), columns=te.columns_)

# Apply Apriori Algorithm
frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)

# Print results
print("Frequent Itemsets:\n", frequent_itemsets)
print("\nAssociation Rules:\n", rules)

6666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666

import pandas as pd
from mlxtend.frequent_patterns import fpgrowth, association_rules
from mlxtend.preprocessing import TransactionEncoder

# Smaller dataset
dataset = [
    ['milk', 'bread'],
    ['milk', 'apple'],
    ['bread', 'apple'],
    ['milk', 'bread', 'apple'],
    ['bread'],
]

# Convert transactions to DataFrame
te = TransactionEncoder()
df = pd.DataFrame(te.fit_transform(dataset), columns=te.columns_)

# Apply FP-Growth Algorithm
frequent_itemsets = fpgrowth(df, min_support=0.4, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)

# Print results
print("Frequent Itemsets:\n", frequent_itemsets)
print("\nAssociation Rules:\n", rules)

7777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777

import pandas as pd
from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules
from mlxtend.preprocessing import TransactionEncoder
import time

# Sample dataset
# Updated sample dataset
dataset = [
    ['milk', 'bread', 'nuts', 'apple'],
    ['milk', 'bread', 'butter'],
    ['milk', 'bread'],
    ['milk', 'apple', 'banana', 'bread'],
    ['bread', 'butter'],
    ['milk', 'banana'],
    ['milk', 'bread', 'butter', 'apple', 'banana', 'nuts'],
    ['bread', 'banana'],
    ['milk', 'bread', 'apple'],
    ['apple', 'banana'],
]


# Convert transactions to DataFrame
te = TransactionEncoder()
df = pd.DataFrame(te.fit_transform(dataset), columns=te.columns_)
# Apply Apriori and FP-Growth
for algo, func in zip(["Apriori", "FP-Growth"], [apriori, fpgrowth]):
    start = time.time()
    frequent_itemsets = func(df, min_support=0.3, use_colnames=True)
    rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)
    print(f"\n{algo} Frequent Itemsets:\n", frequent_itemsets)
    print(f"Execution Time - {algo}: {time.time() - start:.5f} sec")


888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, precision_score, recall_score
import matplotlib.pyplot as plt

# Load and split data
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)

# Train model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))

# Plot decision tree
plt.figure(figsize=(10, 6))
plot_tree(model, filled=True, feature_names=load_iris().feature_names, class_names=load_iris().target_names)
plt.show()

9999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score

# Load Iris dataset
iris = load_iris()
X, y = iris.data, iris.target

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)

# Train Naïve Bayes model
model = GaussianNB()
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)

# Compute metrics

# Clear and obvious output
print("="*30)
print("MODEL USED: Naïve Bayes (GaussianNB)")
print("="*30)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("="*30)


0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load data
X, y = load_iris(return_X_y=True)

# Apply K-Means
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)
labels = kmeans.labels_

# Plot clusters (using only first 2 features for visualization)
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.title("K-Means Clustering (Iris Data)")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
